{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-20T23:34:21.706597Z","iopub.execute_input":"2022-08-20T23:34:21.707256Z","iopub.status.idle":"2022-08-20T23:34:21.735745Z","shell.execute_reply.started":"2022-08-20T23:34:21.707172Z","shell.execute_reply":"2022-08-20T23:34:21.734686Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/spaceship-titanic/sample_submission.csv\n/kaggle/input/spaceship-titanic/train.csv\n/kaggle/input/spaceship-titanic/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#getting our data\ntraining_data = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:37:00.819116Z","iopub.execute_input":"2022-08-20T23:37:00.819449Z","iopub.status.idle":"2022-08-20T23:37:00.885232Z","shell.execute_reply.started":"2022-08-20T23:37:00.819426Z","shell.execute_reply":"2022-08-20T23:37:00.884582Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#splitting our training data\nX = training_data.loc[:, training_data.columns != \"Transported\"]\ny = training_data[\"Transported\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:38:34.939963Z","iopub.execute_input":"2022-08-20T23:38:34.940343Z","iopub.status.idle":"2022-08-20T23:38:34.949001Z","shell.execute_reply.started":"2022-08-20T23:38:34.940317Z","shell.execute_reply":"2022-08-20T23:38:34.948388Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#splitting further into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:41:14.381019Z","iopub.execute_input":"2022-08-20T23:41:14.381373Z","iopub.status.idle":"2022-08-20T23:41:14.404562Z","shell.execute_reply.started":"2022-08-20T23:41:14.381351Z","shell.execute_reply":"2022-08-20T23:41:14.403580Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# getting all preprocessors\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler #to encode categorical variables\nfrom sklearn.impute import SimpleImputer #to impute numerical values\nfrom sklearn.compose import ColumnTransformer #to perform ops of columns seperately\nfrom sklearn.pipeline import Pipeline #this ties the entire thing together to define a pipeline for all processing steps","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:53:54.782633Z","iopub.execute_input":"2022-08-20T23:53:54.782933Z","iopub.status.idle":"2022-08-20T23:53:54.788405Z","shell.execute_reply.started":"2022-08-20T23:53:54.782911Z","shell.execute_reply":"2022-08-20T23:53:54.786317Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#scaleing should be done after imputation\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n#encode first then impute\n#setting unknown value to be an arbitrary one\n#very ineffecient\ncategorical_transformer = Pipeline(steps=[\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=9999)),\n    ('imputer', SimpleImputer(strategy='constant'))\n])","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:54:00.762701Z","iopub.execute_input":"2022-08-20T23:54:00.763020Z","iopub.status.idle":"2022-08-20T23:54:00.768847Z","shell.execute_reply.started":"2022-08-20T23:54:00.762996Z","shell.execute_reply":"2022-08-20T23:54:00.767783Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#splitting the categorical and numerical column names into different lists\nnumeric_columns = X_train.select_dtypes(np.float64).columns\ncategorical_columns = X_train.select_dtypes(object).columns\nnumeric_columns, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:54:34.348156Z","iopub.execute_input":"2022-08-20T23:54:34.348514Z","iopub.status.idle":"2022-08-20T23:54:34.363934Z","shell.execute_reply.started":"2022-08-20T23:54:34.348490Z","shell.execute_reply":"2022-08-20T23:54:34.362590Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(Index(['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], dtype='object'),\n Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP',\n        'Name'],\n       dtype='object'))"},"metadata":{}}]},{"cell_type":"code","source":"#specifying a preprocessor\npreprocessor = ColumnTransformer(\ntransformers = [\n    ('numeric',numeric_transformer,numeric_columns),\n    ('categorical', categorical_transformer, categorical_columns)\n])","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:55:09.291762Z","iopub.execute_input":"2022-08-20T23:55:09.292089Z","iopub.status.idle":"2022-08-20T23:55:09.296388Z","shell.execute_reply.started":"2022-08-20T23:55:09.292065Z","shell.execute_reply":"2022-08-20T23:55:09.295514Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#the pipeline including preprocessing\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n#defining the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('regressor', LinearDiscriminantAnalysis())\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T23:57:28.272419Z","iopub.execute_input":"2022-08-20T23:57:28.272736Z","iopub.status.idle":"2022-08-20T23:57:28.278541Z","shell.execute_reply.started":"2022-08-20T23:57:28.272712Z","shell.execute_reply":"2022-08-20T23:57:28.277296Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#fitting our model\nlinDiscModel = pipeline.fit(X_train,y_train)\nprint(linDiscModel)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:15:14.068493Z","iopub.execute_input":"2022-08-21T00:15:14.068848Z","iopub.status.idle":"2022-08-21T00:15:14.211601Z","shell.execute_reply.started":"2022-08-21T00:15:14.068824Z","shell.execute_reply":"2022-08-21T00:15:14.210712Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('numeric',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], dtype='object')),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OrdinalEncoder(handle_unknown='use_encoded_value',\n                                                                                  unknown_value=9999)),\n                                                                  ('imputer',\n                                                                   SimpleImputer(strategy='constant'))]),\n                                                  Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP',\n       'Name'],\n      dtype='object'))])),\n                ('regressor', LinearDiscriminantAnalysis())])\n","output_type":"stream"}]},{"cell_type":"code","source":"#transforming our test set to be normalized as per our training set\nnormalized_X_test = linDiscModel.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:15:18.297791Z","iopub.execute_input":"2022-08-21T00:15:18.298083Z","iopub.status.idle":"2022-08-21T00:15:18.354023Z","shell.execute_reply.started":"2022-08-21T00:15:18.298059Z","shell.execute_reply":"2022-08-21T00:15:18.352953Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"figure out how to proceed with the normalized test set","metadata":{}},{"cell_type":"code","source":"# getting predictions and testing accuracy\nfrom sklearn.metrics import accuracy_score\npredictions = linDiscModel.predict(X_test)\naccuracy_score(y_test, predictions)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:20:06.071220Z","iopub.execute_input":"2022-08-21T00:20:06.071548Z","iopub.status.idle":"2022-08-21T00:20:06.127381Z","shell.execute_reply.started":"2022-08-21T00:20:06.071524Z","shell.execute_reply":"2022-08-21T00:20:06.126203Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.7388803680981595"},"metadata":{}}]},{"cell_type":"code","source":"# getting cross validation accuracy score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfold = KFold(n_splits=10, random_state=7, shuffle=True)\nresults = cross_val_score(linDiscModel, X_train, y_train, cv=kfold, scoring='accuracy')\nresults.mean()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:22:41.424896Z","iopub.execute_input":"2022-08-21T00:22:41.425236Z","iopub.status.idle":"2022-08-21T00:22:42.956242Z","shell.execute_reply.started":"2022-08-21T00:22:41.425212Z","shell.execute_reply":"2022-08-21T00:22:42.955533Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.7174984875983061"},"metadata":{}}]},{"cell_type":"code","source":"# applying our model to real data\nfinal_predictions = linDiscModel.predict(test_data)\nfinal_predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:35:04.075928Z","iopub.execute_input":"2022-08-21T00:35:04.076272Z","iopub.status.idle":"2022-08-21T00:35:04.146724Z","shell.execute_reply.started":"2022-08-21T00:35:04.076242Z","shell.execute_reply":"2022-08-21T00:35:04.145680Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"array([ True, False,  True, ...,  True, False,  True])"},"metadata":{}}]},{"cell_type":"code","source":"test_data[\"Transported\"] = final_predictions\nsubmissions_final = test_data[[\"PassengerId\",\"Transported\"]]\nsubmissions_final","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:37:17.299744Z","iopub.execute_input":"2022-08-21T00:37:17.300071Z","iopub.status.idle":"2022-08-21T00:37:17.314715Z","shell.execute_reply.started":"2022-08-21T00:37:17.300043Z","shell.execute_reply":"2022-08-21T00:37:17.313902Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"     PassengerId  Transported\n0        0013_01         True\n1        0018_01        False\n2        0019_01         True\n3        0021_01        False\n4        0023_01        False\n...          ...          ...\n4272     9266_02         True\n4273     9269_01        False\n4274     9271_01         True\n4275     9273_01        False\n4276     9277_01         True\n\n[4277 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Transported</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0013_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0018_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0019_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0021_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0023_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4272</th>\n      <td>9266_02</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4273</th>\n      <td>9269_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4274</th>\n      <td>9271_01</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4275</th>\n      <td>9273_01</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4276</th>\n      <td>9277_01</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>4277 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submissions_final.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:37:19.150144Z","iopub.execute_input":"2022-08-21T00:37:19.152088Z","iopub.status.idle":"2022-08-21T00:37:19.163393Z","shell.execute_reply.started":"2022-08-21T00:37:19.152063Z","shell.execute_reply":"2022-08-21T00:37:19.162187Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:31:23.335771Z","iopub.execute_input":"2022-08-21T00:31:23.336107Z","iopub.status.idle":"2022-08-21T00:31:23.670772Z","shell.execute_reply.started":"2022-08-21T00:31:23.336084Z","shell.execute_reply":"2022-08-21T00:31:23.669410Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}